============================================================================================================================
Pods:-

---
apiVersion: v1
kind: Pod                     #it represents creating pods
metadata:                     # information about the pod
  name: vk67220-firstapp
  labels:                     # label is an id for the pod
    app: javawebcontainerapp
spec:                         # container information
  containers:
    - name: javawebcontainer
      image: kastrov/bms
      ports:
        - containerPort: 8080
---

Service:-

---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  selector:
    app: javawebapp
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
---
============================================================================================================================
Here in the below manifest file we are giving nodePort value
============================================================================================================================

---
apiVersion: v1
kind: Pod
metadata:
  name: vk67220-firstapp
  labels:
    app: bmswebapp
spec:
  containers:
    - name: bmswebcontainer
      image: prasad5806/bms:latest
      ports:
        - containerPort: 3000
---

---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
spec:
  type: NodePort
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
      nodePort: 31000
---

============================================================================================================================
Correct Traffic Flow (NodePort / LoadBalancer)
============================================================================================================================
Internet
   |
NodeIP : NodePort (32000)
   |
Service (port: 80)
   |
targetPort (3000)
   |
containerPort (3000)
   |
Application (React / Spring Boot / Node.js)



============================================================================================================================
Here in the above we are writing manifest files for pods and services so here we are writing only one manifest file for both 
============================================================================================================================
---
apiVersion: v1
kind: Pod
metadata:
  name: vk67220-firstapp
  labels:
    app: bmswebapp
spec:
  containers:
    - name: bmswebcontainer
      image: prasad5806/bms:latest
      ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
      nodePort: 31000      (optional nodePort)
---

============================================================================================================================
ClusterIP Service Type
============================================================================================================================
---
apiVersion: v1
kind: Pod
metadata:
  name: vk67220-firstapp
  labels:
    app: bmswebapp
spec:
  containers:
    - name: bmswebcontainer
      image: prasad5806/bms:latest
      ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
spec:
  type: ClusterIP
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
---

============================================================================================================================
NameSpace:-
============================================================================================================================
---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: v1
kind: Pod
metadata:
  name: vk67220-bmsapp
  namespace: vk67220-ns
  labels:
    app: bmswebapp
spec:
  containers:
    - name: bmswebcontainer
      image: prasad5806/bms:latest
      ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
      nodePort: 30777   # optional
---

---
apiVersion: v1
kind: Namespace
metadata:
  name: springboot-ns
---
apiVersion: v1
kind: Pod
metadata:
  name: springboot-app-pod
  namespace: springboot-ns
  labels:
    app: springboot-app
spec:
  containers:
    - name: springboot-container
      image: <your-springboot-image>:latest
      ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: springboot-app-svc
  namespace: springboot-ns
spec:
  type: LoadBalancer
  selector:
    app: springboot-app
  ports:
    - port: 80
      targetPort: 8080
      # nodePort: 30080   # optional
---


============================================================================================================================
ReplicationController
============================================================================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: vk67220-rc
  namespace: vk67220-ns
spec:
  replicas: 3
  selector:
    app: bmswebapp
  template:
    metadata:
      labels:
        app: bmswebapp
    spec:
      containers:
      - name: bmswebappcontainer
        image: prasad5806/bms:latest
        ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31000
---


============================================================================================================================
ReplicaSet
============================================================================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: bmswebapprs
  namespace: vk67220-ns
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bmswebapp
  template:
    metadata:
      labels:
        app: bmswebapp
    spec:
      containers:
        - name: bmswebappcontainer
          image: prasad5806/bms:latest
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
      nodePort: 31000
---


============================================================================================================================
Deployment
============================================================================================================================

RollingUpdate:-

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bmswebapp
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: bmswebapp
  template:
    metadata:
      labels:
        app: bmswebapp
    spec:
      containers:
        - name: bmswebappcontainer
          image: prasad5806/bms:latest
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
---



ReCreate:-

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bmswebapp
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: bmswebapp
  template:
    metadata:
      labels:
        app: bmswebapp
    spec:
      containers:
        - name: bmswebappcontainer
          image: prasad5806/bms:latest
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: bmswebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: bmswebapp
  ports:
    - port: 80
      targetPort: 3000
---

Steps to do :-

Step 1 — Build & Push New Version of Your Application
docker pull prasad5806/bms:latest
docker tag prasad5806/bms:latest prasad5806/bms:v2
docker push prasad5806/bms:v2


Step 2 — Update Deployment to New Version
kubectl set image deployment/bmswebapp \
bmswebappcontainer=prasad5806/bms:v2 \
-n vk67220-ns


Step 3 — Watch Recreate Strategy Behavior
kubectl get pods -n vk67220-ns -w


Step 4 — Check Rollout Status
kubectl rollout status deployment/bmswebapp -n vk67220-ns


Step 5 — Verify Image Version Running
kubectl describe pod <pod-name> -n vk67220-ns | grep Image


Step 6 — If Something Goes Wrong → ROLLBACK (UNDO)
kubectl rollout undo deployment/bmswebapp -n vk67220-ns

Step 7 — Verify You Returned to Older Version
kubectl describe pod <new-pod-name> -n vk67220-ns | grep Image


ReCreate:-

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spotifywebapp
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: spotifywebapp
  template:
    metadata:
      labels:
        app: spotifywebapp
    spec:
      containers:
        - name: spotifywebappcontainer
          image: prasad5806/spotify-app:latest
          ports:
            - containerPort: 5555
---
apiVersion: v1
kind: Service
metadata:
  name: spotifywebappsvc 
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: spotifywebapp
  ports:
    - port: 80
      targetPort: 5555
---


Steps to do :-

Verify image version running
kubectl describe pod spotifywebapp -n vk67220-ns | grep Image

Step 1 — Build & Push New Version of Your Application
docker pull prasad5806/spotify-app:latest
docker tag prasad5806/spotify-app:latest prasad5806/spotify-app:v1
docker push prasad5806/spotify-app:v1

Step 2 — Update Deployment to New Version
kubectl set image deployment/spotifywebappds spotifywebappcontainer=prasad5806/spotify-app:v1 -n vk67220-ns


Step 3 — Watch Recreate Strategy Behavior
kubectl get pods -n vk67220-ns -w


Step 4 — Check Rollout Status
kubectl rollout status deployment/spotifywebappds -n vk67220-ns


Step 5 — Verify Image Version Running
kubectl describe pod spotifywebapp -n vk67220-ns | grep Image

Step 6 — If Something Goes Wrong → ROLLBACK (UNDO)
kubectl rollout undo deployment/spotifywebappds -n vk67220-ns

Step 7 — Verify You Returned to Older Version
kubectl describe pod spotifywebapp -n vk67220-ns | grep Image


RollingUpdate

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spotifywebappds
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: spotifywebapp
  template:
    metadata:
      labels:
        app: spotifywebapp
    spec:
      containers:
        - name: spotifywebappcontainer
          image: prasad5806/spotify-app:latest
          ports:
            - containerPort: 5555
---
apiVersion: v1
kind: Service
metadata:
  name: spotifywebappsvc 
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: spotifywebapp
  ports:
    - port: 80
      targetPort: 5555
---

============================================================================================================================
From docker image we have to push to ECR 
============================================================================================================================

Step 1: Create an ECR repository

aws ecr create-repository \
  --repository-name zomato \
  --region us-east-1

Step 2: Login to ECR

aws ecr get-login-password --region us-east-1 \
| docker login \
--username AWS \
--password-stdin 597918493080.dkr.ecr.us-east-1.amazonaws.com

Step 3: Pull image from Docker Hub

docker pull prasad5806/zomato:latest

Step 4: Tag Docker Hub image for ECR
docker tag prasad5806/zomato:latest \
597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v1

Step 5: Push image to ECR
docker push 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v1

Step 5: Verify image in ECR
aws ecr list-images \
  --repository-name zomato \
  --region us-east-1

Step 6: (VERY IMPORTANT) Ensure EKS nodes can pull ECR images
AmazonEC2ContainerRegistryReadOnly

aws iam list-attached-role-policies \
  --role-name <EKS-node-role-name>

If missing, attach:
aws iam attach-role-policy \
--role-name <EKS-node-role-name> \
--policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zomatowebappds
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: zomatowebapp
  template:
    metadata:
      labels:
        app: zomatowebapp
    spec:
      containers:
        - name: zomatowebappcontainer
          image: 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v1
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: zomatowebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: zomatowebapp
  ports:
    - port: 80
      targetPort: 3000
---




kubectl describe pod zomatowebapp -n vk67220-ns | grep Image

docker pull 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v1
docker tag 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v1 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2
docker push 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2

kubectl set image deployment/zomatowebappds \
zomatowebappcontainer=597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2 \
-n vk67220-ns

kubectl get pods -n vk67220-ns -w

kubectl rollout status deployment/zomatowebappds -n vk67220-ns

kubectl describe pod zomatowebapp -n vk67220-ns | grep Image

kubectl rollout undo deployment/zomatowebappds -n vk67220-ns

kubectl describe pod zomatowebapp -n vk67220-ns | grep Image

============================================================================================================================



============================================================================================================================
Blue-Green Deployment:-
============================================================================================================================

STEP-1: Deploy BLUE (Current Production)

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zomatowebappds
  namespace: vk67220-ns
spec:
  replicas: 5
  selector:
    matchLabels:
      app: zomatowebapp
      version: blue
  template:
    metadata:
      labels:
        app: zomatowebapp
        version: blue
    spec:
      containers:
        - name: zomatowebappcontainer
          image: prasad5806/zomato:latest
          ports:
            - containerPort: 3000

STEP-2: Create Service (points to BLUE)

apiVersion: v1
kind: Service
metadata:
  name: zomatowebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: zomatowebapp
    version: blue
  ports:
  - port: 80
    targetPort: 3000

Apply BLUE

kubectl apply -f zomato-blue.yaml

Verify BLUE

kubectl get pods -n vk67220-ns -l version=blue
kubectl describe pod -l version=blue -n vk67220-ns | grep Image


STEP-3: Prepare GREEN (New Release)

Build & push v2 image

docker build -t zomato:v2 .
docker tag zomato:v2 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2
docker push 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2


STEP-4: Deploy GREEN (WITH NO TRAFFIC)

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zomato-green-ds
  namespace: vk67220-ns
spec:
  replicas: 5
  selector:
    matchLabels:
      app: zomatowebapp
      version: green
  template:
    metadata:
      labels:
        app: zomatowebapp
        version: green
    spec:
      containers:
        - name: zomatowebappcontainer
          image: prasad5806/zomato:v1
          ports:
            - containerPort: 3000
---

Apply

kubectl apply -f zomato-green.yaml

Verify Green

kubectl get pods -n vk67220-ns -l version=green
kubectl describe pod -l version=green -n vk67220-ns | grep Image

STEP-5: TEST GREEN SAFELY

kubectl exec -it <green-pod> -n vk67220-ns -- curl localhost:3000

if the above command not work follow below option

kubectl run curlpod \
  -n vk67220-ns \
  --image=curlimages/curl \
  --rm -it \
  --restart=Never \
  -- curl http://localhost or podip:3000


STEP-6: TRAFFIC SWITCH (THE CRITICAL STEP)

kubectl patch service zomatowebappsvc \
-n vk67220-ns \
-p '{"spec":{"selector":{"app":"zomatowebapp","version":"green"}}}'

STEP-7: VERIFY LIVE VERSION

kubectl get endpoints zomatowebappsvc -n vk67220-ns
kubectl describe pod -l version=green -n vk67220-ns | grep Image

STEP-8: ROLLBACK (IF ISSUE FOUND)

kubectl patch service zomatowebappsvc \
-n vk67220-ns \
-p '{"spec":{"selector":{"app":"zomatowebapp","version":"blue"}}}'

STEP-9: CLEANUP OLD VERSION (OPTIONAL)
kubectl delete deployment zomato-blue -n vk67220-ns

=============================================================================================================================

=============================================================================================================================
Canary Deployment:-
=============================================================================================================================

DEPLOY STABLE VERSION (PRODUCTION)

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zomato-stable
  namespace: vk67220-ns
spec:
  replicas: 9
  selector:
    matchLabels:
      app: zomatowebapp
      release: stable
  template:
    metadata:
      labels:
        app: zomatowebapp
        release: stable
    spec:
      containers:
        - name: zomatowebappcontainer
          image: prasad5806/zomato:latest
          ports:
            - containerPort: 3000
---

Apply

kubectl apply -f zomato-stable.yaml

Verify:

kubectl get pods -n vk67220-ns
kubectl describe pod -l release=stable -n vk67220-ns | grep Image

STEP 3 — CREATE SERVICE (ENTRY POINT)

---
apiVersion: v1
kind: Service
metadata:
  name: zomatowebappsvc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: zomatowebapp   # ❗ NO release here
  ports:
    - port: 80
      targetPort: 3000
---

Apply:-

kubectl apply -f zomato-service.yaml

Verify:-

kubectl get svc -n vk67220-ns

STEP 4 — BUILD & PUSH NEW VERSION (CANARY IMAGE)

docker build -t zomato:v2 .
docker tag zomato:v2 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2
docker push 597918493080.dkr.ecr.us-east-1.amazonaws.com/zomato:v2

STEP 5 — DEPLOY CANARY (SMALL EXPOSURE)

---
apiVersion: v1
kind: Namespace
metadata:
  name: vk67220-ns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zomato-canary
  namespace: vk67220-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zomatowebapp
      release: canary
  template:
    metadata:
      labels:
        app: zomatowebapp
        release: canary
    spec:
      containers:
        - name: zomatowebappcontainer
          image: prasad5806/zomato:v2
          ports:
            - containerPort: 3000
---

Apply:-

kubectl apply -f zomato-canary.yaml


Verify:-

kubectl get pods -n vk67220-ns
kubectl describe pod -l release=canary -n vk67220-ns | grep Image

STEP 6 — MONITOR CANARY (MOST IMPORTANT)

kubectl logs -l release=canary -n vk67220-ns
kubectl describe pod -l release=canary -n vk67220-ns


STEP 7 — DECISION POINT

OPTION A — SUCCESS → PROMOTE CANARY

kubectl scale deployment zomato-canary --replicas=10 -n vk67220-ns
kubectl delete deployment zomato-stable -n vk67220-ns

OPTION B — FAILURE → ROLLBACK

kubectl delete deployment zomato-canary -n vk67220-ns

============================================================================================================================



============================================================================================================================
ConfigMap & Secret
============================================================================================================================

---
apiVersion: v1
kind: Namespace
metadata: 
  name: vk67220-ns
---

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: employee-app-config
  namespace: vk67220-ns
data:
  SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/employees
  POSTGRES_DB: employees
---

---
apiVersion: v1
kind: Secret
metadata:
  name: employee-app-secret
  namespace: vk67220-ns
type: Opaque
data:
  SPRING_DATASOURCE_USERNAME: cG9zdGdyZXM=
  SPRING_DATASOURCE_PASSWORD: cm9vdA==
---

---
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: postgres
  namespace: vk67220-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres 
    spec:
      containers: 
        - name: postgres-container
          image: postgres:13
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: employee-app-secret
                  key: SPRING_DATASOURCE_USERNAME
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: employee-app-secret
                  key: SPRING_DATASOURCE_PASSWORD
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: employee-app-config
                  key: POSTGRES_DB
---
---
apiVersion: v1
kind: Service 
metadata: 
  name: postgres
  namespace: vk67220-ns
spec:
  type: ClusterIP
  selector:
    app: postgres
  ports:
    - port: 5432
      targetPort: 5432
--- 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: employee-app
  namespace: vk67220-ns
spec:
  replicas: 3
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: employee-app 
  template:
    metadata:
      labels:
        app: employee-app
    spec:
      containers:
        - name: employee-app-container
          image: prasad5806/springboot-postgresql-hibernate-crud-example-app:latest
          ports:
            - containerPort: 8080
          env:
            - name: SPRING_DATASOURCE_URL
              valueFrom:
                configMapKeyRef:
                  name: employee-app-config
                  key: SPRING_DATASOURCE_URL

            - name: SPRING_DATASOURCE_USERNAME
              valueFrom:
                secretKeyRef:
                  name: employee-app-secret
                  key: SPRING_DATASOURCE_USERNAME

            - name: SPRING_DATASOURCE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: employee-app-secret
                  key: SPRING_DATASOURCE_PASSWORD
---
---
apiVersion: v1
kind: Service
metadata: 
  name: employee-app-svc
  namespace: vk67220-ns
spec:
  type: LoadBalancer
  selector:
    app: employee-app
  ports:
    - port: 80kubectl get pods -n vk67220-ns -w
      targetPort: 8080
---
===========================================================================================================================

Statefulset

Step 1️⃣ Scale from 1 → 3 replicas
kubectl scale statefulset postgres --replicas=3 -n vk67220-ns

Step 2️⃣ Watch pods come up (ORDERED)
kubectl get pods -n vk67220-ns -w

Step 3️⃣ Verify PVCs
kubectl get pvc -n vk67220-ns

Step 4️⃣ Verify DNS (OptionalStep 7️⃣ Check PVCs after scale down
but powerful)
kubectl run dns-test --image=busybox:1.35 -n vk67220-ns -it --rm -- sh
nslookup postgres-headless
nslookup postgres-1.postgres-headless
nslookup postgres-2.postgres-headless

Step 5️⃣ Scale from 3 → 1 replicas
kubectl scale statefulset postgres --replicas=1 -n vk67220-ns

Step 6️⃣ Watch pods terminate (REVERSE ORDER)
kubectl get pods -n vk67220-ns -w

Step 7️⃣ Check PVCs after scale down
kubectl get pvc -n vk67220-ns

If you intentionally want to remove data for scaled-down pods:
kubectl delete pvc postgres-storage-postgres-1 -n vk67220-ns
kubectl delete pvc postgres-storage-postgres-2 -n vk67220-ns


=============================================================================================================================
Helm Charts 
=============================================================================================================================

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

helm version

helm env

Create a Helm chart

helm create employee-app
cd employee-app

employee-app/
├── Chart.yaml
├── values.yaml
└── templates/
    ├── namespace.yml
    ├── storageclass.yml
    ├── configmap.yml
    ├── secret.yml
    ├── headless_service.yml
    ├── postgresql_statefulset_secret_config_volume.yml
    ├── postgres_service.yml
    ├── app_deployment_config_secret.yml
    └── app_service.yml

Validate Helm chart

helm lint .

helm template employee-app .

Install Helm chart (EXECUTION)

helm install employee-app .

or

helm install employee-app . -f values.yaml

===========================================================================================================================
K8S Autoscaling(HPA)
===========================================================================================================================
Amazon EKS(install metrics first)

helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm install metrics-server metrics-server/metrics-server -n kube-system

Verify

kubectl get apiservices | grep metrics

Expected:
v1beta1.metrics.k8s.io   Available

To test HPA follow below steps 

1)Verify HPA
kubectl get hpa -n vk67220-ns

2)Generate Load (for testing HPA)
kubectl exec -it <employee-app-pod> -n vk67220-ns -- sh -c "while true; do :; done"

3)verify
kubectl get hpa employee-app-hpa -n vk67220-ns -w

4)verify pods 
kubectl get pods -n vk67220 -wkubectl exec -it employee-app-xxx -n vk67220-ns -- sh


===========================================================================================================================
k8s resources 
===========================================================================================================================

To test resources use below commands

kubectl get pod <pod-name> -n vk67220-ns -o yaml | grep -A10 resources

View usage

kubectl top pod -n vk67220-ns

Test CPU limit enforcement
kubectl exec -it employee-app-xxx -n vk67220-ns -- sh
Generate CPU load
yes > /dev/null
Watch CPU
kubectl top pod employee-app-xxx -n vk67220-ns

Test memory limit enforcement
Allocate memory
kubectl exec -it employee-app-xxx -n vk67220-ns -- sh
yes | head -n 500000000 > /tmp/memtest
Check pod status
kubectl get pod -n vk67220-ns

Check events
kubectl describe pod employee-app-xxx -n vk67220-ns

============================================================================================================================
RBAC
============================================================================================================================
ServiceAccount
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: emp-app-sa
  namespace: vk67220-ns
---

Role
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: emp-app-role
  namespace: vk67220-ns
rules:
  - apiGroups: [""]
    resources: ["pods", "services","persistentvolumeclaims","configmaps","secrets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets","statefulsets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["get", "list", "watch"]
---

RoleBinding

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: emp-app-rolebinding
  namespace: vk67220-ns
subjects:
  - kind: ServiceAccount
    name: emp-app-sa
    namespace: vk67220-ns
roleRef:
  kind: Role
  name: emp-app-role
  apiGroup: rbac.authorization.k8s.io
---

template:
  metadata:
    labels:
      app: postgres
  spec:
    serviceAccountName: emp-app-sa
    containers:
      - name: postgres
        image: postgres:13


template:
  metadata:
    labels:
      app: employee-app
  spec:
    serviceAccountName: emp-app-sa
    containers:
      - name: employee-app
        image: prasad5806/springboot-postgresql-hibernate-crud-example-app:latest


Test RBAC using kubectl auth can-i

kubectl auth can-i get pods \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

kubectl auth can-i list services \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

kubectl auth can-i get deployments.apps \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

kubectl auth can-i get statefulsets.apps \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

should return yes


kubectl auth can-i create pods \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

kubectl auth can-i delete deployments.apps \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

kubectl auth can-i apply pods \
  --as=system:serviceaccount:vk67220-ns:emp-app-sa \
  -n vk67220-ns

should return no

kubectl delete pod employee-app-9956d9c7f-mnm44 -n vk67220-ns \     ------> it won't work
--as=system:serviceaccount:vk67220-ns:emp-app-sa

kubectl get pods -n vk67220-ns \                                    ------> it will work
--as=system:serviceaccount:vk67220-ns:emp-app-sa



Create RBAC Test Pod

apiVersion: v1
kind: Pod
metadata:
  name: rbac-test-pod
  namespace: vk67220-ns
spec:
  serviceAccountName: emp-app-sa
  containers:
    - name: kubectl
      image: bitnami/kubectl:latest
      command: ["sleep", "3600"]

kubectl apply -f rbac-test-pod.yml

kubectl get pod rbac-test-pod -n vk67220-ns

kubectl exec -it rbac-test-pod -n vk67220-ns -- /bin/bash

pass

kubectl get pods -n vk67220-ns
kubectl get services -n vk67220-ns
kubectl get deployments -n vk67220-ns
kubectl get statefulsets -n vk67220-ns
kubectl get configmaps -n vk67220-ns
kubectl get secrets -n vk67220-ns
kubectl get hpa -n vk67220-ns

Fails

kubectl create deployment test --image=nginx -n vk67220-ns
kubectl delete pod rbac-test-pod -n vk67220-ns
kubectl apply -f any.yaml


ClusterRole

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vk67220-cluster-readonly
rules:
- apiGroups: [""]
  resources:
    - nodes
    - namespaces
    - persistentvolumes
  verbs: ["get", "list", "watch"]

ClusterRoleBinding

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vk67220-cluster-readonly-binding
subjects:
- kind: Group
  name: dev-team
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: vk67220-cluster-readonly
  apiGroup: rbac.authorization.k8s.io

kubectl edit configmap aws-auth -n kube-system

mapRoles: |
  - rolearn: arn:aws:iam::123456789012:role/dev-role
    username: dev-role
    groups:
      - dev-team


kubectl auth can-i create namespaces --as=dev-role --as-group=dev-team

kubectl auth can-i delete persistentvolumes --as=dev-role --as-group=dev-team


Test Cluster Resources

kubectl get nodes --as=dev-role --as-group=dev-team
kubectl get ns --as=dev-role --as-group=dev-team
kubectl get pv --as=dev-role --as-group=dev-team


Fail

kubectl delete ns vk67220-ns --as=dev-role --as-group=dev-team
kubectl delete pv <pv-name> --as=dev-role --as-group=dev-team


============================================================================================================================
taints and tolerations
============================================================================================================================

STEP1:-Choose one node for DB
kubectl get nodes -o wide

kubectl get nodes --show-labels

Add Label to DB node
kubectl label node ip-10-0-1-24.ec2.internal role=db

Verify label:
kubectl get nodes --show-labels | grep role

STEP2:-Add a taint to DB Node
kubectl taint nodes ip-10-0-1-24.ec2.internal dedicated=db:NoSchedule

STEP3:- apply all yaml files 

STEP4:- 
kubectl get pods -n vk67220-ns -o wide
kubectl describe pod postgres-0 -n vk67220-ns | grep -i toleration -A3
kubectl describe pod postgres-0 -n vk67220-ns | grep -i node


==========================================================================================================================
Ingress Controller
==========================================================================================================================

kubectl get nodes

kubectl get nodes --show-labels | grep role

kubectl label node ip-10-0-1-58.ec2.internal role=db

kubectl taint nodes ip-10-0-1-58.ec2.internal dedicated=db:NoSchedule

now run all yaml files

kubectl get pods -n vk67220-ns -o wide
kubectl describe pod postgres-0 -n vk67220-ns | grep -i toleration -A3
kubectl describe pod postgres-0 -n vk67220-ns | grep -i node

Install NGINX Ingress Controller (AWS)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/aws/deploy.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/aws/deploy.yaml

kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx
kubectl get svc ingress-nginx-controller -n ingress-nginx


vi ingress.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: employee-app-ingress
  namespace: vk67220-ns
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: techsolutions.devopstechz.store
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: employee-app-svc
            port:
              number: 80


Kastro ingress.yml file 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: techsolutions-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - techsolutions.yourdomain.com
    secretName: techsolutions-tls
  rules:
  - host: techsolutions.yourdomain.com
    http:
      paths:
      - path: /(.*)
        pathType: Prefix
        backend:
          service:
            name: techsolutions-service
            port:
              number: 80
  - http:
      paths:
      - path: /(.*)
        pathType: Prefix
        backend:
          service:
            name: techsolutions-service
            port:
              number: 80



kubectl get svc ingress-nginx-controller -n ingress-nginx

Hostinger DNS Setup

Type	    Name	               Points to
CNAME	 techsolutions	   a1b2c3d4e5f6.elb.amazonaws.com

nslookup techsolutions.devopstechz.store

kubectl get ingress -n vk67220-ns
kubectl describe ingress employee-app-ingress -n vk67220-ns

go inside the db pod
kubectl exec -it postgres-0 -n vk67220-ns -- sh

CREATE TABLE employees (
    id SERIAL PRIMARY KEY,
    email_address VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100)
);

INSERT INTO employees (email_address, first_name, last_name)
VALUES ('john.doe@gmail.com', 'John', 'Doe');


Test working or not
curl -H "Host: techsolutions.devopstechz.store" http://<LB-DNS>/api/v1/employees
browser 
http://techsolutions.devopstechz.store/api/v1/employees/
